<!DOCTYPE html>
                                <html>
                                <head>
                                 <title>DonkeyFeed search results</title>
                                 </head>
                                <body>
                                <p><h1>Search terms found: ['artificial intelligence', 'OpenAI', 'Mistral']</h1></p>
                                <p><h2>Here are your search results for today:</h2></p>
                                <p>You ran this search on 2024_02_27_1544.</p>
                               <h3>Microsoft made a $16 million investment in Mistral AI</h3><p><a href="https://techcrunch.com/2024/02/27/microsoft-made-a-16-million-investment-in-mistral-ai/">https://techcrunch.com/2024/02/27/microsoft-made-a-16-million-investment-in-mistral-ai/</a></p><p><p>Yesterday, Mistral AI, a Paris-based AI startup working on foundational models, announced a new large language model that could rival OpenAI’s GPT-4, a chat assistant and a distribution partnership with Microsoft. But Microsoft and Mistral AI buried the news —&#160;or at least an important part. As part of the partnership, Microsoft is investing €15 million [&#8230;]</p>
<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Alibaba staffer offers a glimpse into building LLMs in China</h3><p><a href="https://techcrunch.com/2024/02/27/alibaba-staff-offers-glimpse-into-life-of-llm-researcher-in-china/">https://techcrunch.com/2024/02/27/alibaba-staff-offers-glimpse-into-life-of-llm-researcher-in-china/</a></p><p><p>Chinese tech companies are gathering all sorts of resources and talent to narrow their gap with OpenAI, and experiences for researchers on both sides of the Pacific Ocean can be surprisingly similar. A recent X post from an Alibaba researcher offers a rare glimpse into the life of developing Large Language Models at the e-commerce [&#8230;]</p>
<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Microsoft made a $16 million investment in Mistral AI</h3><p><a href="https://techcrunch.com/2024/02/27/microsoft-made-a-16-million-investment-in-mistral-ai/">https://techcrunch.com/2024/02/27/microsoft-made-a-16-million-investment-in-mistral-ai/</a></p><p><p>Yesterday, Mistral AI, a Paris-based AI startup working on foundational models, announced a new large language model that could rival OpenAI’s GPT-4, a chat assistant and a distribution partnership with Microsoft. But Microsoft and Mistral AI buried the news —&#160;or at least an important part. As part of the partnership, Microsoft is investing €15 million [&#8230;]</p>
<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>How to Use ChatGPT’s Memory Feature</h3><p><a href="https://www.wired.com/story/how-to-use-chatgpt-memory-feature/">https://www.wired.com/story/how-to-use-chatgpt-memory-feature/</a></p><p>The latest update to OpenAI’s chatbot improves the AI’s ability to remember user details, but the feature is not yet available for all ChatGPT accounts.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Move Over OpenAI, Microsoft Has a New Darling Startup</h3><p><a href="https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307">https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307</a></p><p><img class="type:primaryImage" src="https://i.kinja-img.com/image/upload/c_fit,q_80,w_636/adacc2b211f120f0045ea62db34e8b51.jpg" /><p>Microsoft <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/" rel="noopener noreferrer" target="_blank">announced</a> a new partnership with Mistral AI on Monday, a <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217">French startup</a> whose advanced large language models rival <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889">OpenAI’s ChatGPT</a> and <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/copilot-pro-20-a-month-microsoft-1851163966">Microsoft Copilot</a>. Mistral also released <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://mistral.ai/news/mistral-large/" rel="noopener noreferrer" target="_blank">Mistral Large</a>, its most advanced language model to date, and “<a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://mistral.ai/news/le-chat-mistral/" rel="noopener noreferrer" target="_blank">le Chat</a>,” a chat interface built on top of Mistral’s models.</p><p><a href="https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307">Read more...</a></p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Things You Didn’t Know About Tesla’s Humanoid Robots</h3><p><a href="https://gizmodo.com/things-you-didnt-know-teslas-elon-musk-humanoid-robots-1851278709">https://gizmodo.com/things-you-didnt-know-teslas-elon-musk-humanoid-robots-1851278709</a></p><p><img class="type:primaryImage" src="https://i.kinja-img.com/image/upload/c_fit,q_80,w_636/29787fab5f40690d4e7391e48482f30e.jpg" /><p>Elon Musk’s Tesla is famous for its electric cars, but the company is also a world leader in robotics. Musk considers Tesla’s AI to be severely underrated—a claim that’s not entirely baseless. While Tesla attracts world-class talent to build robots and artificial intelligence, these areas are much less talked about…</p><p><a href="https://gizmodo.com/things-you-didnt-know-teslas-elon-musk-humanoid-robots-1851278709">Read more...</a></p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>U.S. Is Reportedly Using AI to Decide Where to Drop Bombs</h3><p><a href="https://gizmodo.com/artificial-intelligence-pentagon-bombs-middle-east-1851287514">https://gizmodo.com/artificial-intelligence-pentagon-bombs-middle-east-1851287514</a></p><p><img class="type:primaryImage" src="https://i.kinja-img.com/image/upload/c_fit,q_80,w_636/c2c7e3dcf547c83615379837ade96506.jpg" /><p>If you’ve been wondering when our military would start <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/ai-deployed-nukes-have-peace-world-tense-war-simulation-1851234455">deploying artificial intelligence on the battlefield</a>, the answer is that it appears to be already happening. The Pentagon has been using computer vision algorithms to help identify targets for airstrikes, according to a <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://www.bloomberg.com/news/articles/2024-02-26/us-says-it-used-ai-to-help-find-targets-it-hit-in-iraq-syria-and-yemen?utm_campaign=socialflow-organic&amp;utm_content=tv&amp;utm_medium=social&amp;cmpid%3D=socialflow-twitter-tv&amp;utm_source=twitter&amp;sref=P6Q0mxvj" rel="noopener noreferrer" target="_blank">report</a> from Bloomberg News. Just recently,…</p><p><a href="https://gizmodo.com/artificial-intelligence-pentagon-bombs-middle-east-1851287514">Read more...</a></p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Move Over OpenAI, Microsoft Has a New Darling Startup</h3><p><a href="https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307">https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307</a></p><p><img class="type:primaryImage" src="https://i.kinja-img.com/image/upload/c_fit,q_80,w_636/adacc2b211f120f0045ea62db34e8b51.jpg" /><p>Microsoft <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/" rel="noopener noreferrer" target="_blank">announced</a> a new partnership with Mistral AI on Monday, a <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/mistral-artificial-intelligence-gpt-3-openai-1851091217">French startup</a> whose advanced large language models rival <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/chatgpt-gone-berserk-giving-nonsensical-responses-1851273889">OpenAI’s ChatGPT</a> and <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://gizmodo.com/copilot-pro-20-a-month-microsoft-1851163966">Microsoft Copilot</a>. Mistral also released <a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://mistral.ai/news/mistral-large/" rel="noopener noreferrer" target="_blank">Mistral Large</a>, its most advanced language model to date, and “<a class="sc-1out364-0 dPMosf sc-145m8ut-0 lcFFec js_link" href="https://mistral.ai/news/le-chat-mistral/" rel="noopener noreferrer" target="_blank">le Chat</a>,” a chat interface built on top of Mistral’s models.</p><p><a href="https://gizmodo.com/move-over-openai-microsoft-has-new-darling-startup-1851287307">Read more...</a></p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Stable Diffusion 3 is a new AI image generator that won't mess up text in pictures, its makers claim</h3><p><a href="https://www.engadget.com/stable-diffusion-3-is-a-new-ai-image-generator-that-wont-mess-up-text-in-pictures-its-makers-claim-233751335.html?src=rss">https://www.engadget.com/stable-diffusion-3-is-a-new-ai-image-generator-that-wont-mess-up-text-in-pictures-its-makers-claim-233751335.html?src=rss</a></p><p><p>Stability AI, the startup behind Stable Diffusion, the tool that uses generative AI to create images from text prompts, <a class="no-affiliate-link" href="https://stability.ai/news/stable-diffusion-3">revealed</a> Stable Diffusion 3, a next-generation model, on Thursday. Stability AI claimed that the new model, which isn’t widely available yet, improves image quality, works better with prompts containing multiple subjects, and can more accurate text as part of the generated image, something that previous Stable Diffusion models weren’t great at.<br /><br />Stability AI CEO Emad Mosque posted some examples of this on X.<br /></p>
<div id="860ca04ec9204bbabfa63e147f62b83a"><blockquote class="twitter-tweet"><p dir="ltr" lang="en"><a href="https://twitter.com/hashtag/SD3?src=hash&amp;ref_src=twsrc%5Etfw">#SD3</a> can do quite a lot of text… <a href="https://t.co/DfcUzOZymj">https://t.co/DfcUzOZymj</a></p>— Emad (@EMostaque) <a href="https://twitter.com/EMostaque/status/1760785067557720122?ref_src=twsrc%5Etfw">February 22, 2024</a></blockquote>
 

</div>
<p><br />The announcement comes days after Stability AI’s largest rival, OpenAI, <a class="no-affiliate-link" href="https://www.engadget.com/openais-new-sora-model-can-generate-minute-long-videos-from-text-prompts-195717694.html">unveiled Sora</a>, a brand new AI model capable of generating nearly-realistic, high-definition videos from simple text prompts. Sora, which isn’t available to the general public yet either, sparked concerns about its potential to create realistic-looking fake footage. OpenAI said it's working with experts in misinformation and hateful content to test the tool before making it widely available.Stability AI said it’s doing the same. “[We] have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3 by bad actors,” the company wrote in a blog post on its website. “By continually collaborating with researchers, experts, and our community, we expect to innovate further with integrity as we approach the model’s public release.” <br /><br />It’s not clear when Stable Diffusion 3 will be released to the public, but until then, anyone interested can join <a class="no-affiliate-link" href="http://stability.ai/stablediffusion3">a waitlist</a>.</p>
<span id="end-legacy-contents"></span>This article originally appeared on Engadget at https://www.engadget.com/stable-diffusion-3-is-a-new-ai-image-generator-that-wont-mess-up-text-in-pictures-its-makers-claim-233751335.html?src=rss</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Google pauses Gemini’s ability to generate people after overcorrecting for diversity in historical images</h3><p><a href="https://www.engadget.com/google-pauses-geminis-ability-to-generate-people-after-overcorrecting-for-diversity-in-historical-images-220303074.html?src=rss">https://www.engadget.com/google-pauses-geminis-ability-to-generate-people-after-overcorrecting-for-diversity-in-historical-images-220303074.html?src=rss</a></p><p><p>Google said Thursday it’s pausing its <a href="https://www.engadget.com/googles-gemini-15-pro-is-a-new-more-efficient-ai-model-181909354.html">Gemini chatbot’s</a> ability to generate people. The move comes after viral social posts showed the AI tool overcorrecting for diversity, <a href="https://www.engadget.com/google-promises-to-fix-geminis-image-generation-following-complaints-that-its-woke-073445160.html">producing “historical” images of Nazis, America’s Founding Fathers and the Pope </a>as people of color.</p>
<p>“We’re already working to address recent issues with Gemini’s image generation feature,” Google <a class="no-affiliate-link" href="https://twitter.com/Google_Comms/status/1760603321944121506">posted</a> on X (<a class="no-affiliate-link" href="https://www.nytimes.com/2024/02/22/technology/google-gemini-german-uniforms.html">via</a> <em>The New York Times</em>). “While we do this, we’re going to pause the image generation of people and will re-release an improved version soon.”</p>
<span id="end-legacy-contents"></span><p>The X user @JohnLu0x <a class="no-affiliate-link" href="https://twitter.com/JohnLu0x/status/1760170103474356519">posted</a> screenshots of Gemini’s results for the prompt, “Generate an image of a 1943 German Solidier.” (Their misspelling of “Soldier” was intentional to trick the AI into bypassing its content filters to generate otherwise blocked Nazi images.) The generated results appear to show Black, Asian and Indigenous soldiers wearing Nazi uniforms.</p>
<div id="92a1c683d3fa403283ccbd47ed774119"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Still real real broke <a href="https://t.co/FrPBrYi47v">pic.twitter.com/FrPBrYi47v</a></p>— John L. (@JohnLu0x) <a href="https://twitter.com/JohnLu0x/status/1760381145882177923?ref_src=twsrc%5Etfw">February 21, 2024</a></blockquote> </div> 
<p>Other social users criticized Gemini for producing images for the prompt, “Generate a glamour shot of a [ethnicity] couple.” It successfully spit out images when using “Chinese,” “Jewish” or “South African” prompts but refused to produce results for “white.” “I cannot fulfill your request due to the potential for perpetuating harmful stereotypes and biases associated with specific ethnicities or skin tones,” Gemini responded to the latter request.</p>
<p>“John L.,” who helped kickstart the backlash, <a class="no-affiliate-link" href="https://twitter.com/JohnLu0x/status/1760170103474356519">theorizes</a> that Google applied a well-intended but lazily tacked-on solution to a real problem. “Their system prompt to add diversity to portrayals of people isn’t very smart (it doesn’t account for gender in historically male roles like pope; doesn’t account for race in historical or national depictions),” the user posted. After the internet’s anti-“woke” brigade latched onto their posts, the user clarified that they support diverse representation but believe Google’s “stupid move” was that it failed to do so “in a nuanced way.”</p>
<p>Before pausing Gemini’s ability to produce people, Google wrote, “We’re working to improve these kinds of depictions immediately. Gemini’s Al image generation does generate a wide range of people. And that’s generally a good thing because people around the world use it. But it’s missing the mark here.”</p>
<p>The episode could be seen as a (much less subtle) callback to <a href="https://www.engadget.com/google-bard-ai-hands-on-a-work-in-progress-with-plenty-of-caveats-170956025.html">the launch of Bard in 2023</a>. Google’s original AI chatbot got off to a rocky start when an advertisement for the chatbot on Twitter (now X) included an <a href="https://www.engadget.com/google-bard-chatbot-false-information-twitter-ad-165533095.html">inaccurate “fact” about the James Webb Space Telescope</a>.</p>
<p>As Google often does, it rebranded Bard in hopes of giving it a fresh start. Coinciding with a big performance and feature update, the company <a href="https://www.engadget.com/google-rebrands-its-bard-ai-chatbot-as-gemini-which-now-has-its-own-android-app-151303210.html">renamed the chatbot Gemini</a> earlier this month as the company races to hold its ground against <a href="https://www.engadget.com/how-openais-chatgpt-has-changed-the-world-in-just-a-year-140050053.html">OpenAI’s ChatGPT</a> and <a href="https://www.engadget.com/microsoft-copilot-heres-everything-you-need-to-know-about-the-companys-ai-assistant-130004909.html">Microsoft Copilot</a> —&nbsp;both of which pose an existential threat to its search engine (and, therefore, advertising revenue).</p>This article originally appeared on Engadget at https://www.engadget.com/google-pauses-geminis-ability-to-generate-people-after-overcorrecting-for-diversity-in-historical-images-220303074.html?src=rss</p><p>---------------------------------</p></body>
                                </html>
                                <h3>The Pentagon used Project Maven-developed AI to identify air strike targets</h3><p><a href="https://www.engadget.com/the-pentagon-used-project-maven-developed-ai-to-identify-air-strike-targets-103940709.html?src=rss">https://www.engadget.com/the-pentagon-used-project-maven-developed-ai-to-identify-air-strike-targets-103940709.html?src=rss</a></p><p><p>The US military has ramped up its use of artificial intelligence tools after the October 7 Hamas attacks on Israel, based on a new report by <a href="https://www.bloomberg.com/news/articles/2024-02-26/us-says-it-used-ai-to-help-find-targets-it-hit-in-iraq-syria-and-yemen?utm_campaign=socialflow-organic&amp;utm_content=tv&amp;utm_medium=social&amp;cmpid%3D=socialflow-twitter-tv&amp;utm_source=twitter&amp;sref=10lNAhZ9"><em>Bloomberg</em></a>. Schuyler Moore, US Central Command's chief technology officer, told the news organization that machine learning algorithms helped the Pentagon identify targets for more than 85 air strikes in the Middle East this month.&nbsp;</p>
<p>US bombers and fighter aircraft carried out those air strikes against seven facilities in Iraq and Syria on February 2, fully destroying or at least damaging rockets, missiles, drone storage facilities and militia operations centers. The Pentagon had also used AI systems to find rocket launchers in Yemen and surface combatants in the Red Sea, which it had then destroyed through multiple air strikes in the same month.</p>
<span id="end-legacy-contents"></span><p>The machine learning algorithms used to narrow down targets were developed under Project Maven, Google's now-defunct partnership the Pentagon. To be precise, the project entailed the use of Google's artificial intelligence technology by the US military to analyze drone footage and flag images for further human review. It caused an uproar among Google employees: Thousands had <a href="https://www.engadget.com/2018-05-14-google-project-maven-employee-protest.html">petitioned</a> the company to end its partnership with Pentagon, and some even quit over its involvement altogether. A few months after that employee protest, Google decided <a href="https://www.engadget.com/2018-06-26-pentagon-official-project-maven-alarmed-google-withdrawal.html">not to renew</a> its contract, which had ended in 2019.&nbsp;</p>
<p>Moore told <em>Bloomberg</em> that US forces in the Middle East haven't stopped experimenting with the use of algorithms to identify potential targets using drone or satellite imagery even after Google ended its involvement. The military has been testing out their use over the past year in digital exercises, she said, but it started using targeting algorithms in actual operations after the October 7 Hamas attacks. She clarified, however, that human workers constantly checked and verified the AI systems' target recommendations. Human personnel were also the ones who proposed how to stage the attacks and which weapons to use. &quot;There is never an algorithm that’s just running, coming to a conclusion and then pushing onto the next step,&quot; she said. &quot;Every step that involves AI has a human checking in at the end.&quot;</p>This article originally appeared on Engadget at https://www.engadget.com/the-pentagon-used-project-maven-developed-ai-to-identify-air-strike-targets-103940709.html?src=rss</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Google apologizes for ahistorical and inaccurate Gemini AI images: ‘We missed the mark’</h3><p><a href="https://venturebeat.com/ai/google-apologizes-for-ahistorical-and-inaccurate-gemini-ai-images-we-missed-the-mark/">https://venturebeat.com/ai/google-apologizes-for-ahistorical-and-inaccurate-gemini-ai-images-we-missed-the-mark/</a></p><p>After Google's AI chatbot Gemini generated embarrassing and inaccurate images of historical figures, the company apologized and paused the feature, capping a rocky start to its AI ambitions versus rivals like OpenAI.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Mistral partners with Microsoft, announces new large AI model with chat app</h3><p><a href="https://venturebeat.com/ai/mistral-partners-with-microsoft-announces-new-large-ai-model-with-chat-app/">https://venturebeat.com/ai/mistral-partners-with-microsoft-announces-new-large-ai-model-with-chat-app/</a></p><p>In MMLU tests, Mistral Large had an accuracy of 81.2%, sitting behind GPT-4’s, but doing much better than Llama 2 70B and Gemini Pro 1.0.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Nvidia boss sees AI 'tipping point' as sales soar</h3><p><a href="https://www.bbc.co.uk/news/business-68366467">https://www.bbc.co.uk/news/business-68366467</a></p><p>The artificial intelligence boom has helped Nvidia become one of the most valuable firms in the US.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>How AI is helping the search for extraterrestrial life</h3><p><a href="https://www.bbc.co.uk/news/business-68346015">https://www.bbc.co.uk/news/business-68346015</a></p><p>Artificial intelligence software is being used to look for signs of alien lifeforms.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Why Microsoft Has Accepted Unions, Unlike Its Rivals</h3><p><a href="https://www.nytimes.com/2024/02/25/business/economy/microsoft-corporate-progressive-labor.html">https://www.nytimes.com/2024/02/25/business/economy/microsoft-corporate-progressive-labor.html</a></p><p>Once again a juggernaut thanks to artificial intelligence, Microsoft has worked to shed its strong-arm image. Is there a catch?</p><p>---------------------------------</p></body>
                                </html>
                                <h3>China’s Rush to Dominate A.I. Comes With a Twist: It Depends on U.S. Technology</h3><p><a href="https://www.nytimes.com/2024/02/21/technology/china-united-states-artificial-intelligence.html">https://www.nytimes.com/2024/02/21/technology/china-united-states-artificial-intelligence.html</a></p><p>China’s tech firms were caught off guard by breakthroughs in generative artificial intelligence. Beijing’s regulations and a sagging economy aren’t helping.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Video Friday: Pedipulate</h3><p><a href="https://spectrum.ieee.org/video-friday-pedipulate">https://spectrum.ieee.org/video-friday-pedipulate</a></p><p><img src="https://spectrum.ieee.org/media-library/image.png?id=51541316&amp;width=1245&amp;height=700&amp;coordinates=0%2C74%2C0%2C75" /><br /><br /><p>Video Friday is your weekly selection of awesome robotics videos, collected by your friends at <em>IEEE Spectrum</em> robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please <a href="mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday">send us your events</a> for inclusion.<br /></p><h5><a href="https://humanrobotinteraction.org/2024/">HRI 2024</a>: 11–15 March 2024, BOULDER, COLO.</h5><h5><a href="https://www.eurobot.org/">Eurobot Open 2024</a>: 8–11 May 2024, LA ROCHE-SUR-YON, FRANCE</h5><h5><a href="https://2024.ieee-icra.org/">ICRA 2024</a>: 13–17 May 2024, YOKOHAMA, JAPAN</h5><h5><a href="https://2024.robocup.org/">RoboCup 2024</a>: 17–22 July 2024, EINDHOVEN, NETHERLANDS</h5><p>Enjoy today’s videos!</p><div class="horizontal-rule"></div><div><span style="display: none;"> </span></div><blockquote class="rm-anchors" id="gd4wyjpxqtu"><em>Legged robots have the potential to become vital in maintenance, home support, and exploration scenarios. In order to interact with and manipulate their environments, most legged robots are equipped with a dedicated robot arm, which means additional mass and mechanical complexity compared to standard legged robots. In this work, we explore pedipulation—using the legs of a legged robot for manipulation.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>This work, by Philip Arm, Mayank Mittal, Hendrik Kolvenbach, and Marco Hutter from ETH Zurich’s Robotic Systems Lab, will be presented at the IEEE International Conference on Robotics and Automation (<a href="https://2024.ieee-icra.org/" target="_blank">ICRA 2024</a>) in May, in Japan (see events calendar above).</p><p>[ <a href="https://sites.google.com/leggedrobotics.com/pedipulate">Pedipulate</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="vixalvxtqpg">I learned a new word today: “stigmergy.” Stigmergy is a kind of group coordination that’s based on environmental modification. Like, when insects leave pheromone trails, they’re not directly sending messages to other individuals. But as a group, ants are able to manifest surprisingly complex coordinated behaviors. Cool, right? Researchers at IRIDIA are exploring the possibilities for robots using stigmergy with a cool “artificial pheromone” system using a UV-sensitive surface.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>“Automatic Design of Stigmergy-Based Behaviors for Robot Swarms,” by Muhammad Salman, David Garzón Ramos, and Mauro Birattari, is published in the journal <em>Communications Engineering</em>.</p><p>[ <a href="https://www.nature.com/articles/s44172-024-00175-7"><em>Nature</em></a> ] via [ <a href="https://iridia.ulb.ac.be/~mbiro/habanero/">IRIDIA</a> ]</p><p>Thanks, David!</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="xfkfojbei-g"><em>Filmed in July 2017, this video shows Atlas walking through a “hatch” on a pitching surface. This skill uses autonomous behaviors, with the robot not knowing about the rocking world. Robot built by Boston Dynamics for the DARPA Robotics Challenge in 2013. Software by IHMC Robotics.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://robots.ihmc.us/">IHMC</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ek_yuhc_css">That IHMC video reminded me of the SAFFiR program for <a href="https://spectrum.ieee.org/virginia-techs-robots-will-save-you-from-disasters" target="_blank">Shipboard Autonomous Firefighting Robots</a>, which is responsible for a bunch of really cool research in partnership with the <a href="https://www.nrl.navy.mil/" target="_blank">U.S. Naval Research Laboratory</a>. NRL did some interesting stuff with <a href="https://spectrum.ieee.org/tag/nexi" target="_blank">Nexi robots</a> from MIT and made their own videos. That effort I think didn’t get nearly enough credit for being very entertaining while communicating important robotics research.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.nrl.navy.mil/itd/aic/">NRL</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="oq2udtyd2jw">I want more robot videos with this energy.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.csail.mit.edu/research/distributed-robotics-laboratory">MIT CSAIL</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="4az8qzfbjha"><em>Large industrial-asset operators increasingly use robotics to automate hazardous work at their facilities. This has led to soaring demand for autonomous inspection solutions like ANYmal. Series production by our partner Zollner enables ANYbotics to supply our customers with the required quantities of robots.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.anybotics.com/robotics/anymal/">ANYbotics</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="xrzlbkshv_s">This week is <a href="https://mediahub.unl.edu/media/21920" target="_blank">Grain Bin Safety Week</a>, and Grain Weevil is here to help.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.grainweevil.com/">Grain Weevil</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="ql34bxzb-70">Oof, this is some heavy, heavy deep-time stuff.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.anybotics.com/news/autonomous-inspection-for-enhanced-nuclear-safety-at-onkalo/">Onkalo</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="rw94qcrhe9q">And now, this.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://www.youtube.com/@RozenZebet">RozenZebet</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="pdamz3oyugc"><em>Hawkeye is a real-time multimodal conversation-and-interaction agent for the Boston Dynamics’ mobile robot Spot. Leveraging OpenAI’s experimental GPT-4 Turbo and Vision AI models, Hawkeye aims to empower everyone, from seniors to health care professionals in forming new and unique interactions with the world around them.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>That moment at 1:07 is so relatable.</p><p>[ <a href="https://github.com/darryltanzil/spot-boston-dynamics">Hawkeye</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="sposwz_4zvo">Wing would really prefer that if you find one of their drones on the ground, you don’t run off with it.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://wing.com/">Wing</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="wprufur-zn8"><em>The rover Artemis, developed at the DFKI Robotics Innovation Center, has been equipped with a penetrometer that measures the soil’s penetration resistance to obtain precise information about soil strength. The video showcases an initial test run with the device mounted on the robot. During this test, the robot was remotely controlled, and the maximum penetration depth was limited to 15 millimeters.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://robotik.dfki-bremen.de/en/research/robot-systems/artemis">DFKI</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="yr7xzgwidme"><em>To efficiently achieve complex humanoid loco-manipulation tasks in industrial contexts, we propose a combined vision-based tracker-localization interplay integrated as part of a task-space whole-body-optimization control. Our approach allows humanoid robots, targeted for industrial manufacturing, to manipulate and assemble large-scale objects while walking.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://hal.science/hal-04125159">Paper</a> ]</p><div class="horizontal-rule"></div><blockquote class="rm-anchors" id="ctydxeegbbu"><em>We developed a novel multibody robot (called the Two-Body Bot) consisting of two small-footprint mobile bases connected by a four-bar linkage where handlebars are mounted. Each base measures only 29.2 centimeters wide, making the robot likely the slimmest ever developed for mobile postural assistance.</em></blockquote><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://darbelofflab.mit.edu/nri-eldercare/#handle-anywhere">MIT</a> ]</p><div class="horizontal-rule"></div><p class="rm-anchors" id="5vnbbcm_zyq">Lex Fridman interviews Marc Raibert.</p><p class="shortcode-media shortcode-media-youtube"><span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span></p><p>[ <a href="https://lexfridman.com/marc-raibert/">Lex Fridman</a> ]</p><div class="horizontal-rule"></div></p><p>---------------------------------</p></body>
                                </html>
                                <h3>What Is Generative AI?</h3><p><a href="https://spectrum.ieee.org/what-is-generative-ai">https://spectrum.ieee.org/what-is-generative-ai</a></p><p><img src="https://spectrum.ieee.org/media-library/an-animation-showing-a-conceptual-representation-of-a-generative-ai-network.gif?id=51461227&amp;width=1245&amp;height=700&amp;coordinates=0%2C42%2C0%2C43" /><br /><br /><p>Generative AI is today’s buzziest form of artificial intelligence, and it’s what powers chatbots like <a href="https://spectrum.ieee.org/chatbot-chatgpt-interview" target="_self">ChatGPT</a>, <a href="https://www.youtube.com/watch?v=ZLTpvQpY95w" target="_blank">Ernie</a>, <a href="https://spectrum.ieee.org/llama-2-llm" target="_self">LLaMA</a>, <a href="https://www.anthropic.com/news/introducing-claude" target="_blank">Claude</a>, and <a href="https://cohere.com/models/command" target="_blank">Command</a>—as well as <a href="https://spectrum.ieee.org/ai-design" target="_self">image generators</a> like <a href="https://spectrum.ieee.org/openai-dall-e-2" target="_self">DALL-E 2</a>, <a href="https://stability.ai/stable-image" target="_blank">Stable Diffusion</a>, <a href="https://www.adobe.com/products/firefly.html" target="_blank">Adobe Firefly</a>, and <a href="https://spectrum.ieee.org/tag/midjourney" target="_self">Midjourney</a>. Generative AI is the branch of AI that enables machines to learn patterns from vast datasets and then to autonomously produce new content based on those patterns. Although generative AI is fairly new, there are already many examples of models that can produce text, images, videos, and audio.</p><p>
	Many “<a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">foundation models</a>” have been trained on enough data to be competent in a wide variety of tasks. For example, a large language model can generate essays, <a href="https://spectrum.ieee.org/ai-programming" target="_blank">computer code</a>, recipes, <a href="https://spectrum.ieee.org/ai-protein-design" target="_blank">protein structures</a>, jokes, <a href="https://spectrum.ieee.org/chatgpt-medical-exam" target="_blank">medical diagnostic advice</a>, and <a href="https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/" target="_blank">much more</a>. It can also theoretically generate instructions for building a bomb or creating a bioweapon, though safeguards are supposed to prevent such types of misuse.
</p><h2>What’s the difference between AI, machine learning, and generative AI?</h2><p>
	Artificial intelligence (AI) refers to a wide variety of computational approaches to mimicking human intelligence. 
	<a href="https://spectrum.ieee.org/can-machine-learning-teach-us-anything" target="_self">Machine learning</a> (ML) is a subset of AI; it focuses on algorithms that enable systems to learn from data and improve their performance. Before generative AI came along, most ML models learned from datasets to perform tasks such as classification or prediction. Generative AI is a specialized type of ML involving models that perform the task of generating new content, venturing into the realm of creativity.
</p><h2>What architectures do generative AI models use?</h2><p>
	Generative models are built using a variety of neural network architectures—essentially the design and structure that defines how the model is organized and how information flows through it. Some of the most well-known architectures are 
	<a href="https://en.wikipedia.org/wiki/Variational_autoencoder" rel="noopener noreferrer" target="_blank">variational autoencoders</a> (VAEs), <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="noopener noreferrer" target="_blank">generative adversarial networks</a> (GANs), and <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener noreferrer" target="_blank">transformers</a>. It’s the transformer architecture, first shown in this seminal <a href="https://arxiv.org/abs/1706.03762" rel="noopener noreferrer" target="_blank">2017 paper from Google</a>, that powers today’s large language models. However, the transformer architecture is less suited for other types of generative AI, such as image and audio generation.</p><p>
	Autoencoders learn efficient representations of data through an 
	<a href="https://www.cloudskillsboost.google/course_templates/543" rel="noopener noreferrer" target="_blank">encoder-decoder framework</a>. The encoder compresses input data into a lower-dimensional space, known as the <a href="https://en.wikipedia.org/wiki/Latent_space" rel="noopener noreferrer" target="_blank">latent (or embedding) space</a>, that preserves the most essential aspects of the data. A decoder can then use this compressed representation to reconstruct the original data. Once an autoencoder has been trained in this way, it can use novel inputs to generate what it considers the appropriate outputs. These models are often deployed in <a href="https://medium.com/@judyyes10/generate-images-using-variational-autoencoder-vae-4d429d9bdb5" rel="noopener noreferrer" target="_blank">image-generation tools</a> and have also found use in <a href="https://keras.io/examples/generative/molecule_generation/" rel="noopener noreferrer" target="_blank">drug discovery</a>, where they can be used to generate new molecules with desired properties.
</p><p>
	With generative adversarial networks (GANs), the training involves a 
	<a href="https://developers.google.com/machine-learning/gan/generator" rel="noopener noreferrer" target="_blank">generator and a discriminator</a> that can be considered adversaries. The generator strives to create realistic data, while the discriminator aims to distinguish between those generated outputs and real “ground truth” outputs. Every time the discriminator catches a generated output, the generator uses that feedback to try to improve the quality of its outputs. But the discriminator also receives feedback on its performance. This adversarial interplay results in the refinement of both components, leading to the generation of increasingly authentic-seeming content. GANs are best known for <a href="https://www.sciencedirect.com/science/article/pii/S1877050923001916" rel="noopener noreferrer" target="_blank">creating deepfakes</a> but can also be used for more benign forms of image generation and many other applications.
</p><p>
	The transformer is arguably the reigning champion of generative AI architectures for its ubiquity in today’s powerful large language models (LLMs). Its strength lies in its attention mechanism, which enables the model to focus on different parts of an input sequence while making predictions. In the case of language models, the input consists of strings of words that make up sentences, and the transformer predicts what words will come next (we’ll get into the details below). In addition, transformers can process all the elements of a sequence in parallel rather than marching through it from beginning to end, as earlier types of models did; this 
	<a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="noopener noreferrer" target="_blank">parallelization</a> makes training faster and more efficient. When developers added vast datasets of text for transformer models to learn from, today’s remarkable chatbots emerged.
</p><h2>How do large language models work?</h2><p>
	A transformer-based LLM is trained by giving it a vast dataset of text to learn from. The attention mechanism comes into play as it processes sentences and looks for patterns. By looking at all the words in a sentence at once, it gradually begins to understand which words are most commonly found together and which words are most important to the meaning of the sentence. It learns these things by trying to predict the next word in a sentence and comparing its guess to the ground truth. Its errors act as feedback signals that cause the model to adjust the weights it assigns to various words before it tries again.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" style="float: left;">
<img alt="A chart shows the size of five LLMs in parameters and their performance on a benchmark." class="rm-shortcode rm-resized-image" id="91ba0" src="https://spectrum.ieee.org/media-library/a-chart-shows-the-size-of-five-llms-in-parameters-and-their-performance-on-a-benchmark.png?id=51455578&amp;width=980" />
<small class="image-media media-caption">These five LLMs vary greatly in size (given in parameters), and the larger models have better performance on a standard LLM benchmark test. </small><small class="image-media media-photo-credit">IEEE Spectrum</small>
</p><p>
	To explain the training process in slightly more technical terms, the text in the training data is broken down into elements called 
	<a href="https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/" rel="noopener noreferrer" target="_blank">tokens</a>, which are words or pieces of words—but for simplicity’s sake, let’s say all tokens are words. As the model goes through the sentences in its training data and learns the relationships between tokens, it creates a list of numbers, called a vector, for each one. All the numbers in the vector represent various aspects of the word: its semantic meanings, its relationship to other words, its frequency of use, and so on. Similar words, like <em>elegant </em>and <em>fancy</em>, will have similar vectors and will also be near each other in the vector space. These vectors are called word embeddings. The parameters of an LLM include the weights associated with all the word embeddings and the attention mechanism. <a href="https://spectrum.ieee.org/tag/gpt-4" target="_blank">GPT-4</a>, the OpenAI model that’s considered the current champion, is rumored to have more than 1 trillion parameters. </p><p>
	Given enough data and training time, the LLM begins to understand the subtleties of language. While much of the training involves looking at text sentence by sentence, the attention mechanism also captures relationships between words throughout a longer text sequence of many paragraphs. Once an LLM is trained and is ready for use, the attention mechanism is still in play. When the model is generating text in response to a prompt, it’s using its predictive powers to decide what the next word should be. When generating longer pieces of text, it predicts the next word in the context of all the words it has written so far; this function increases the coherence and continuity of its writing.
</p><h2>Why do large language models hallucinate?</h2><p>
	You may have heard that LLMs sometimes “<a href="https://spectrum.ieee.org/ai-hallucination" target="_self">hallucinate</a>.” That’s a polite way to say they make stuff up very convincingly. A model sometimes generates text that fits the context and is grammatically correct, yet the material is erroneous or nonsensical. This bad habit stems from LLMs training on vast troves of data drawn from the Internet, plenty of which is not factually accurate. Since the model is simply trying to predict the next word in a sequence based on what it has seen, it may generate plausible-sounding text that has no grounding in reality.
</p><h2>Why is generative AI controversial?</h2><p>
	One source of controversy for generative AI is the provenance of its training data. Most AI companies that train large models to generate text, images, video, and audio have 
	<a href="https://spectrum.ieee.org/ai-ethics" target="_self">not been transparent</a> about the content of their training datasets. Various leaks and experiments have revealed that those datasets <a href="https://spectrum.ieee.org/midjourney-copyright" target="_self">include copyrighted material</a> such as books, newspaper articles, and movies. A <a href="https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30" rel="noopener noreferrer" target="_blank">number</a> of <a href="https://www.reuters.com/legal/john-grisham-other-top-us-authors-sue-openai-over-copyrights-2023-09-20/" rel="noopener noreferrer" target="_blank">lawsuits</a> are underway to determine whether <a href="https://spectrum.ieee.org/generative-ai-ip-problem" target="_self">use of copyrighted material</a> for training AI systems constitutes fair use, or whether the AI companies need to pay the copyright holders for use of their material.
</p><p>
	On a related note, many people are concerned that the widespread use of generative AI <a href="https://spectrum.ieee.org/ai-taking-over-jobs" target="_blank">will take jobs away</a> from creative humans who make art, music, written works, and so forth. People are also concerned that it could take jobs from humans who do a wide range of white-collar jobs, including translators, paralegals, customer-service representatives, and journalists. There have already been a few 
	<a href="https://www.washingtonpost.com/technology/2024/01/10/duolingo-ai-layoffs/" rel="noopener noreferrer" target="_blank">troubling layoffs</a>, but it’s hard to say yet whether generative AI will be reliable enough for large-scale enterprise applications. (See above about hallucinations.)</p><p>
	Finally, there’s the danger that generative AI will be used to make bad stuff. And there are of course many categories of bad stuff it could theoretically be used for. Generative AI can be used for personalized scams and phishing attacks: For example, using “voice cloning,” scammers can 
	<a href="https://www.cbsnews.com/news/scammers-ai-mimic-voices-loved-ones-in-distress/" rel="noopener noreferrer" target="_blank">copy the voice of a specific person</a> and call the person’s family with a plea for help (and money). All formats of generative AI—text, audio, image, and video—can be used to generate misinformation by creating plausible-seeming representations of things that never happened, which is a particularly worrying possibility when it comes to <a href="https://spectrum.ieee.org/deepfakes-election" target="_self">elections</a>. (Meanwhile, as <em>IEEE </em><em>Spectrum</em> reported this week, the U.S. Federal Communications Commission has responded by <a href="https://spectrum.ieee.org/ai-robocalls-2667266649" target="_blank">outlawing AI-generated robocalls</a>.) Image- and video-generating tools can be used to produce nonconsensual pornography, although the tools made by mainstream companies disallow such use. And chatbots can theoretically walk a would-be terrorist through the steps of making a bomb, nerve gas, and a host of other horrors. Although the big LLMs have safeguards to prevent such misuse, some hackers delight in circumventing those safeguards. What’s more, “uncensored” versions of <a href="https://spectrum.ieee.org/open-source-ai-2666932122" target="_self">open-source LLMs</a> are out there.
</p><p>
	Despite such potential problems, many people think that generative AI can also make people more productive and could be used as a tool to enable entirely new forms of creativity. We’ll likely see both disasters and creative flowerings and plenty else that we don’t expect. But knowing the basics of how these models work is increasingly crucial for tech-savvy people today. Because no matter how sophisticated these systems grow, it’s the humans’ job to keep them running, make the next ones better, and with any luck, help people out too.
</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Let Robots Do Your Lab Work</h3><p><a href="https://spectrum.ieee.org/air-force-research-ares-os">https://spectrum.ieee.org/air-force-research-ares-os</a></p><p><img src="https://spectrum.ieee.org/media-library/image.webp?id=51522099&amp;width=980" /><br /><br />
<p><strong>Dina Genkina:</strong> Hi. I’m <a href="https://spectrum.ieee.org/u/dina_genkina" target="_self">Dina Genkina</a> for <em>IEEE Spectrum</em>‘s <em>Fixing the Future</em>. Before we start, I want to tell you that you can get the latest coverage from some of Spectrum’s most important beeps, including AI, Change, and Robotics, by signing up for one of our free newsletters. Just go to <a href="https://spectrum.ieee.org/newsletters/" target="_self">spectrum.ieee.org\newsletters</a> to subscribe. Today, a guest is Dr. <a href="https://www.linkedin.com/in/benji-maruyama-563b57b8/" rel="noopener noreferrer" target="_blank">Benji Maruyama,</a> a Principal Materials Research Engineer at the <a href="https://www.afrl.af.mil/" rel="noopener noreferrer" target="_blank">Air Force Research Laboratory</a>, or AFRL. Dr. Maruyama is a materials scientist, and his research focuses on carbon nanotubes and making research go faster. But he’s also a man with a dream, a dream of a world where science isn’t something done by a select few locked away in an ivory tower, but something most people can participate in. He hopes to start what he calls the billion scientist movement by building AI-enabled research robots that are accessible to all. Benji, thank you for coming on the show.</p><p>Benji Maruyama: Thanks, Dina. Great to be with you. I appreciate the invitation.</p><p>Genkina: Yeah. So let’s set the scene a little bit for our listeners. So you advocate for this billion scientist movement. If everything works amazingly, what would this look like? Paint us a picture of how AI will help us get there.</p><p>Maruyama: Right, great. Thanks. Yeah. So one of the things as you set the scene there is right now, to be a scientist, most people need to have access to a big lab with very expensive equipment. So I think top universities, government labs, industry folks, lots of equipment. It’s like a million dollars, right, to get one of them. And frankly, just not that many of us have access to those kinds of instruments. But at the same time, there’s probably a lot of us who want to do science, right? And so how do we make it so that anyone who wants to do science can try, can have access to instruments so that they can contribute to it. So that’s the basics behind citizen science or democratization of science so that everyone can do it. And one way to think of it is what happened with <a href="https://spectrum.ieee.org/tag/3d-printing" target="_self">3D printing</a>. It used to be that in order to make something, you had to have access to a machine shop or maybe get fancy tools and dyes that could cost tens of thousands of dollars a pop. Or if you wanted to do electronics, you had to have access to very expensive equipment or services. But when 3D printers came along and became very inexpensive, all of a sudden now, anyone with access to a 3D printer, so maybe in a school or a library or a makerspace could print something out. And it could be something fun, like a game piece, but it could also be something that got you to an invention, something that was maybe useful to the community, was either a prototype or an actual working device.</p><p>And so really, 3D printing democratized manufacturing, right? It made it so that many more of us could do things that before only a select few could. And so that’s where we’re trying to go with science now, is that instead of only those of us who have access to big labs, we’re building research robots. And when I say we, we’re doing it, but now there are a lot of others who are doing it as well, and I’ll get into that. But the example that we have is that we took a 3D printer that you can buy off the internet for less than $300. Plus a couple of extra parts, a webcam, a Raspberry Pi board, and a tripod really, so only four components. You can get them all for $300. Load them with open-source software that was developed by AFIT, the <a href="https://www.afit.edu/" rel="noopener noreferrer" target="_blank">Air Force Institute of Technology</a>. So <a href="https://www.afit.edu/BIOS/bio.cfm?facID=172" rel="noopener noreferrer" target="_blank">Burt Peterson</a> and Greg Captain [inaudible]. We worked together to build this fully autonomous 3D printing robot that taught itself how to print to better than manufacturer’s specifications. So that was a really fun advance for us, and now we’re trying to take that same idea and broaden it. So I’ll turn it back over to you.</p><p>Genkina: Yeah, okay. So maybe let’s talk a little bit about this automated research robot that you’ve made. So right now, it works with a 3D printer, but is the big picture that one day it’s going to give people access to that million dollar lab? How would that look like?</p><p>Maruyama: Right, so there are different models out there. One, we just did a workshop at the University of— sorry, North Carolina State University about that very problem, right? So there’s two models. One is to get low-cost scientific tools like the 3D printer. There’s a couple of different chemistry robots, one out of University of Maryland and NIST, one out of University of Washington that are in the sort of 300 to 1,000 dollars range that makes it accessible. The other part is kind of the user facility model. So in the US, the Department of Energy National Labs have many user facilities where you can apply to get time on very expensive instruments. Now we’re talking tens of millions. For example, Brookhaven has a synchrotron light source where you can sign up and it doesn’t cost you any money to use the facility. And you can get days on that facility. And so that’s already there, but now the advances are that by using this, autonomy, autonomous closed loop experimentation, that the work that you do will be much faster and much more productive. So, for example, on ARES, our Autonomous Research System at AFRL, we actually were able to do experiments so fast that a professor who came into my lab said, it just took me aside and said, “Hey, Benji, in a week’s worth of time, I did a dissertation’s worth of research.” So maybe five years worth of research in a week. So imagine if you keep doing that week after week after week, how fast research goes. So it’s very exciting.</p><p>Genkina: Yeah, so tell us a little bit about how that works. So what’s this system that has sped up five years of research into a week and made graduate students obsolete? Not yet, not yet. How does that work? Is that the 3D printer system or is that a—</p><p>Maruyama: So we started with our system to grow carbon nanotubes. And I’ll say, actually, when we first thought about it, your comment about graduate students being absolute— obsolete, sorry, is interesting and important because, when we first built our system that worked it 100 times faster than normal, I thought that might be the case. We called it sort of graduate student out of the loop. But when I started talking with people who specialize in autonomy, it’s actually the opposite, right? It’s actually empowering graduate students to go faster and also to do the work that they want to do, right? And so just to digress a little bit, if you think about farmers before the Industrial Revolution, what were they doing? They were plowing fields with oxen and beasts of burden and hand plows. And it was hard work. And now, of course, you wouldn’t ask a farmer today to give up their tractor or their combine harvester, right? They would say, of course not. So very soon, we expect it to be the same for researchers, that if you asked a graduate student to give up their autonomous research robot five years from now, they’ll say, “Are you crazy? This is how I get my work done.”</p><p>But for our original ARES system, it worked on the synthesis of <a href="https://spectrum.ieee.org/tag/carbon-nanotubes" target="_self">carbon nanotubes</a>. So that meant that what we’re doing is trying to take this system that’s been pretty well studied, but we haven’t figured out how to make it at scale. So at hundreds of millions of tons per year, sort of like polyethylene production. And part of that is because it’s slow, right? One experiment takes a day, but also because there are just so many different ways to do a reaction, so many different combinations of temperature and pressure and a dozen different gases and half the periodic table as far as the catalyst. It’s just too much to just brute force your way through. So even though we went from experiments where we could do 100 experiments a day instead of one experiment a day, just that combinatorial space was vastly overwhelmed our ability to do it, even with many research robots or many graduate students. So the idea of having artificial intelligence algorithms that drive the research is key. And so that ability to do an experiment, see what happened, and then analyze it, iterate, and constantly be able to choose the optimal next best experiment to do is where ARES really shines. And so that’s what we did. ARES taught itself how to grow carbon nanotubes at controlled rates. And we were the first ones to do that for material science in our 2016 publication.</p><p>Genkina: That’s very exciting. So maybe we can peer under the hood a little bit of this AI model. How does the magic work? How does it pick the next best point to take and why it’s better than you could do as a graduate student or researcher?</p><p>Maruyama: Yeah, and so I think it’s interesting, right? In science, a lot of times we’re taught to hold everything constant, change one variable at a time, search over that entire space, see what happened, and then go back and try something else, right? So we reduce it to one variable at a time. It’s a reductionist approach. And that’s worked really well, but a lot of the problems that we want to go after are simply too complex for that reductionist approach. And so the benefit of being able to use artificial intelligence is that high dimensionality is no problem, right? Tens of dimensions search over very complex high-dimensional parameter space, which is overwhelming to humans, right? Is just basically bread and butter for AI. The other part to it is the iterative part. The beauty of doing autonomous experimentation is that you’re constantly iterating. You’re constantly learning over what just happened. You might also say, well, not only do I know what happened experimentally, but I have other sources of prior knowledge, right? So for example, ideal gas law says that this should happen, right? Or Gibbs phase rule might say, this can happen or this can’t happen. So you can use that prior knowledge to say, “Okay, I’m not going to do those experiments because that’s not going to work. I’m going to try here because this has the best chance of working.”</p><p>And within that, there are many different machine learning or artificial intelligence algorithms. Bayesian optimization is a popular one to help you choose what experiment is best. There’s also new AI that people are trying to develop to get better search.</p><p>Genkina: Cool. And so the software part of this autonomous robot is available for anyone to download, which is also really exciting. So what would someone need to do to be able to use that? Do they need to get a 3D printer and a Raspberry Pi and set it up? And what would they be able to do with it? Can they just build carbon nanotubes or can they do more stuff?</p><p>Maruyama: Right. So what we did, we built ARES OS, which is our open source software, and we’ll make sure to get you <a href="https://github.com/AFRL-ARES/ARES_OS" rel="noopener noreferrer" target="_blank">the GitHub link</a> so that anyone can download it. And the idea behind ARES OS is that it provides a software framework for anyone to build their own autonomous research robot. And so the 3D printing example will be out there soon. But it’s the starting point. Of course, if you want to build your own new kind of robot, you still have to do the software development, for example, to link the ARES framework, the core, if you will, to your particular hardware, maybe your particular camera or 3D printer, or pipetting robot, or spectrometer, whatever that is. We have examples out there and we’re hoping to get to a point where it becomes much more user-friendly. So having direct Python connects so that you don’t— currently it’s programmed in C#. But to make it more accessible, we’d like it to be set up so that if you can do Python, you can probably have good success in building your own research robot.</p><p>Genkina: Cool. And you’re also working on a educational version of this, I understand. So what’s the status of that and what’s different about that version?</p><p>Maruyama: Yeah, right. So the educational version is going to be-- its sort of composition of a combination of hardware and software. So what we’re starting with is a low-cost 3D printer. And we’re collaborating now with the <a href="https://engineering.buffalo.edu/materials-design-innovation.html" rel="noopener noreferrer" target="_blank">University at Buffalo, Materials Design Innovation Department</a>. And we’re hoping to build up a robot based on a 3D printer. And we’ll see how it goes. It’s still evolving. But for example, it could be based on this very inexpensive $200 3D printer. It’s an <a href="https://store.creality.com/collections/ender-series-3d-printer" rel="noopener noreferrer" target="_blank">Ender 3D printer</a>. There’s another printer out there that’s based on <a href="https://jubilee3d.com/index.php?title=Main_Page" rel="noopener noreferrer" target="_blank">University of Washington’s Jubilee printer</a>. And that’s a very exciting development as well. So professors <a href="https://mse.washington.edu/facultyfinder/lilo-pozzo" rel="noopener noreferrer" target="_blank">Lilo Pozzo</a> and <a href="https://www.hcde.washington.edu/peek" rel="noopener noreferrer" target="_blank">Nadya Peek</a> at the University of Washington built this Jubilee robot with that idea of accessibility in mind. And so combining our ARES OS software with their Jubilee robot hardware is something that I’m very excited about and hope to be able to move forward on.</p><p>Genkina: What’s this Jubilee 3D printer? How is it different from a regular 3D printer?</p><p>Maruyama: It’s very open source. Not all 3D printers are open source and it’s based on a gantry system with interchangeable heads. So for example, you can get not just a 3D printing head, but other heads that might do things like do indentation, see how stiff something is, or maybe put a camera on there that can move around. And so it’s the flexibility of being able to pick different heads dynamically that I think makes it super useful. For the software, right, we have to have a good, accessible, user-friendly graphical user interface, a GUI. That takes time and effort, so we want to work on that. But again, that’s just the hardware software. Really to make ARES a good educational platform, we need to make it so that a teacher who’s interested can have the lowest activation barrier possible, right? We want she or he to be able to pull a lesson plan off of the internet, have supporting YouTube videos, and actually have the material that is a fully developed curriculum that’s mapped against state standards.</p><p>So that, right now, if you’re a teacher who— let’s face it, teachers are already overwhelmed with all that they have to do, putting something like this into their curriculum can be a lot of work, especially if you have to think about, well, I’m going to take all this time, but I also have to meet all of my teaching standards, all the state curriculum standards. And so if we build that out so that it’s a matter of just looking at the curriculum and just checking off the boxes of what state standards it maps to, then that makes it that much easier for the teacher to teach.</p><p>Genkina: Great. And what do you think is the timeline? Do you expect to be able to do this sometime in the coming year?</p><p>Maruyama: That’s right. These things always take longer than hoped for than expected, but we’re hoping to do it within this calendar year and very excited to get it going. And I would say for your listeners, if you’re interested in working together, please let me know. We’re very excited about trying to involve as many people as we can.</p><p>Genkina: Great. Okay, so you have the educational version, and you have the more research geared version, and you’re working on making this educational version more accessible. Is there something with the research version that you’re working on next, how you’re hoping to upgrade it, or is there something you’re using it for right now that you’re excited about?</p><p>There’s a number of things that we are very excited about the possibility of carbon nanotubes being produced at very large scale. So right now, people may remember carbon nanotubes as that great material that sort of never made it and was very overhyped. But there’s a core group of us who are still working on it because of the important promise of that material. So it’s material that is super strong, stiff, lightweight, electrically conductive. Much better than silicon as a digital electronics compute material. All of those great things, except we’re not making it at large enough scale. It’s actually used pretty significantly in lithium-ion batteries. It’s an important application. But other than that, it’s sort of like where’s my flying car? It’s never panned out. But there’s, as I said, a group of us who are working to really produce carbon nanotubes at much larger scale. So large scale for nanotubes now is sort of in the kilogram or ton scale. But what we need to get to is hundreds of millions of tons per year production rates. And why is that? Well, there’s a great effort that came out of <a href="https://spectrum.ieee.org/tag/Arpa-e" target="_self">ARPA-E</a>. So the Department of Energy Advanced Research Projects Agency and the E is for Energy in that case.</p><p>So they funded a collaboration between Shell Oil and Rice University to pyrolyze methane, so natural gas into hydrogen for the hydrogen economy. So now that’s a clean burning fuel plus carbon. And instead of burning the carbon to CO2, which is what we now do, right? We just take natural gas and feed it through a turbine and generate electric power instead of— and that, by the way, generates so much CO2 that it’s causing global climate change. So if we can do that pyrolysis at scale, at hundreds of millions of tons per year, it’s literally a save the world proposition, meaning that we can avoid so much CO2 emissions that we can reduce global CO2 emissions by 20 to 40 percent. And that is the save the world proposition. It’s a huge undertaking, right? That’s a big problem to tackle, starting with the science. We still don’t have the science to efficiently and effectively make carbon nanotubes at that scale. And then, of course, we have to take the material and turn it into useful products. So the batteries is the first example, but thinking about replacing copper for electrical wire, replacing steel for structural materials, aluminum, all those kinds of applications. But we can’t do it. We can’t even get to that kind of development because we haven’t been able to make the carbon nanotubes at sufficient scale.</p><p>So I would say that’s something that I’m working on now that I’m very excited about and trying to get there, but it’s going to take some good developments in our research robots and some very smart people to get us there.</p><p>Genkina: Yeah, it seems so counterintuitive that making everything out of carbon is good for lowering carbon emissions, but I guess that’s the break.</p><p>Maruyama: Yeah, it is interesting, right? So people talk about carbon emissions, but really, the molecule that’s causing global warming is carbon dioxide, CO2, which you get from burning carbon. And so if you take that methane and parallelize it to carbon nanotubes, that carbon is now sequestered, right? It’s not going off as CO2. It’s staying in solid state. And not only is it just not going up into the atmosphere, but now we’re using it to replace steel, for example, which, by the way, steel, aluminum, copper production, all of those things emit lots of CO2 in their production, right? They’re energy intensive as a material production. So it’s kind of ironic.</p><p>Genkina: Okay, and are there any other research robots that you’re excited about that you think are also contributing to this democratization of science process?</p><p>Maruyama: Yeah, so we talked about Jubilee, the NIST robot, which is from Professor Ichiro Takeuchi at Maryland and Gilad Kusne at NIST, National Institute of Standards and Technology. Theirs is fun too. It’s LEGO as. So it’s actually based on a LEGO robotics platform. So it’s an actual chemistry robot built out of Legos. So I think that’s fun as well. And you can imagine, just like we have LEGO robot competitions, we can have autonomous research robot competitions where we try and do research through these robots or competitions where everybody sort of starts with the same robot, just like with LEGO robotics. So that’s fun as well. But I would say there’s a growing number of people doing these kinds of, first of all, low-cost science, accessible science, but in particular low-cost autonomous experimentation.</p><p>Genkina: So how far are we from a world where a high school student has an idea and they can just go and carry it out on some autonomous research system at some high-end lab?</p><p>Maruyama: That’s a really good question. I hope that it’s going to be in 5 to 10 years, that it becomes reasonably commonplace. But it’s going to take still some significant investment to get this going. And so we’ll see how that goes. But I don’t think there are any scientific impediments to getting this done. There is a significant amount of engineering to be done. And sometimes we hear, oh, it’s just engineering. The engineering is a significant problem. And it’s work to get some of these things accessible, low cost. But there are lots of great efforts. There are people who have used CDs, compact discs to make spectrometers out of. There are lots of good examples of citizen science out there. But it’s, I think, at this point, going to take investment in software, in hardware to make it accessible, and then importantly, getting students really up to speed on what AI is and how it works and how it can help them. And so I think it’s actually really important. So again, that’s the democratization of science is if we can make it available to everyone and accessible, then that helps people, everyone contribute to science. And I do believe that there are important contributions to be made by ordinary citizens, by people who aren’t you know PhDs working in a lab.</p><p>And I think there’s a lot of science out there to be done. If you ask working scientists, almost no one has run out of ideas or things they want to work on. There’s many more scientific problems to work on than we have the time where people are funding to work on. And so if we make science cheaper to do, then all of a sudden, more people can do science. And so those questions start to be resolved. And so I think that’s super important. And now we have, instead of, just those of us who work in big labs, you have millions, tens of millions, up to a billion people, that’s the billion scientist idea, who are contributing to the scientific community. And that, to me, is so powerful that many more of us can contribute than just the few of us who do it right now.</p><p>Genkina: Okay, that’s a great place to end on, I think. So, today we spoke to Dr. Benji Maruyama, a material scientist at AFRL, about his efforts to democratize scientific discovery through automated research robots. For IEEE Spectrum, I’m Dina Genkina, and I hope you’ll join us next time on Fixing the Future.</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>What Is Generative AI?</h3><p><a href="https://spectrum.ieee.org/what-is-generative-ai">https://spectrum.ieee.org/what-is-generative-ai</a></p><p><img src="https://spectrum.ieee.org/media-library/an-animation-showing-a-conceptual-representation-of-a-generative-ai-network.gif?id=51461227&amp;width=1245&amp;height=700&amp;coordinates=0%2C42%2C0%2C43" /><br /><br /><p>Generative AI is today’s buzziest form of artificial intelligence, and it’s what powers chatbots like <a href="https://spectrum.ieee.org/chatbot-chatgpt-interview" target="_self">ChatGPT</a>, <a href="https://www.youtube.com/watch?v=ZLTpvQpY95w" target="_blank">Ernie</a>, <a href="https://spectrum.ieee.org/llama-2-llm" target="_self">LLaMA</a>, <a href="https://www.anthropic.com/news/introducing-claude" target="_blank">Claude</a>, and <a href="https://cohere.com/models/command" target="_blank">Command</a>—as well as <a href="https://spectrum.ieee.org/ai-design" target="_self">image generators</a> like <a href="https://spectrum.ieee.org/openai-dall-e-2" target="_self">DALL-E 2</a>, <a href="https://stability.ai/stable-image" target="_blank">Stable Diffusion</a>, <a href="https://www.adobe.com/products/firefly.html" target="_blank">Adobe Firefly</a>, and <a href="https://spectrum.ieee.org/tag/midjourney" target="_self">Midjourney</a>. Generative AI is the branch of AI that enables machines to learn patterns from vast datasets and then to autonomously produce new content based on those patterns. Although generative AI is fairly new, there are already many examples of models that can produce text, images, videos, and audio.</p><p>
	Many “<a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">foundation models</a>” have been trained on enough data to be competent in a wide variety of tasks. For example, a large language model can generate essays, <a href="https://spectrum.ieee.org/ai-programming" target="_blank">computer code</a>, recipes, <a href="https://spectrum.ieee.org/ai-protein-design" target="_blank">protein structures</a>, jokes, <a href="https://spectrum.ieee.org/chatgpt-medical-exam" target="_blank">medical diagnostic advice</a>, and <a href="https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/" target="_blank">much more</a>. It can also theoretically generate instructions for building a bomb or creating a bioweapon, though safeguards are supposed to prevent such types of misuse.
</p><h2>What’s the difference between AI, machine learning, and generative AI?</h2><p>
	Artificial intelligence (AI) refers to a wide variety of computational approaches to mimicking human intelligence. 
	<a href="https://spectrum.ieee.org/can-machine-learning-teach-us-anything" target="_self">Machine learning</a> (ML) is a subset of AI; it focuses on algorithms that enable systems to learn from data and improve their performance. Before generative AI came along, most ML models learned from datasets to perform tasks such as classification or prediction. Generative AI is a specialized type of ML involving models that perform the task of generating new content, venturing into the realm of creativity.
</p><h2>What architectures do generative AI models use?</h2><p>
	Generative models are built using a variety of neural network architectures—essentially the design and structure that defines how the model is organized and how information flows through it. Some of the most well-known architectures are 
	<a href="https://en.wikipedia.org/wiki/Variational_autoencoder" rel="noopener noreferrer" target="_blank">variational autoencoders</a> (VAEs), <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="noopener noreferrer" target="_blank">generative adversarial networks</a> (GANs), and <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener noreferrer" target="_blank">transformers</a>. It’s the transformer architecture, first shown in this seminal <a href="https://arxiv.org/abs/1706.03762" rel="noopener noreferrer" target="_blank">2017 paper from Google</a>, that powers today’s large language models. However, the transformer architecture is less suited for other types of generative AI, such as image and audio generation.</p><p>
	Autoencoders learn efficient representations of data through an 
	<a href="https://www.cloudskillsboost.google/course_templates/543" rel="noopener noreferrer" target="_blank">encoder-decoder framework</a>. The encoder compresses input data into a lower-dimensional space, known as the <a href="https://en.wikipedia.org/wiki/Latent_space" rel="noopener noreferrer" target="_blank">latent (or embedding) space</a>, that preserves the most essential aspects of the data. A decoder can then use this compressed representation to reconstruct the original data. Once an autoencoder has been trained in this way, it can use novel inputs to generate what it considers the appropriate outputs. These models are often deployed in <a href="https://medium.com/@judyyes10/generate-images-using-variational-autoencoder-vae-4d429d9bdb5" rel="noopener noreferrer" target="_blank">image-generation tools</a> and have also found use in <a href="https://keras.io/examples/generative/molecule_generation/" rel="noopener noreferrer" target="_blank">drug discovery</a>, where they can be used to generate new molecules with desired properties.
</p><p>
	With generative adversarial networks (GANs), the training involves a 
	<a href="https://developers.google.com/machine-learning/gan/generator" rel="noopener noreferrer" target="_blank">generator and a discriminator</a> that can be considered adversaries. The generator strives to create realistic data, while the discriminator aims to distinguish between those generated outputs and real “ground truth” outputs. Every time the discriminator catches a generated output, the generator uses that feedback to try to improve the quality of its outputs. But the discriminator also receives feedback on its performance. This adversarial interplay results in the refinement of both components, leading to the generation of increasingly authentic-seeming content. GANs are best known for <a href="https://www.sciencedirect.com/science/article/pii/S1877050923001916" rel="noopener noreferrer" target="_blank">creating deepfakes</a> but can also be used for more benign forms of image generation and many other applications.
</p><p>
	The transformer is arguably the reigning champion of generative AI architectures for its ubiquity in today’s powerful large language models (LLMs). Its strength lies in its attention mechanism, which enables the model to focus on different parts of an input sequence while making predictions. In the case of language models, the input consists of strings of words that make up sentences, and the transformer predicts what words will come next (we’ll get into the details below). In addition, transformers can process all the elements of a sequence in parallel rather than marching through it from beginning to end, as earlier types of models did; this 
	<a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="noopener noreferrer" target="_blank">parallelization</a> makes training faster and more efficient. When developers added vast datasets of text for transformer models to learn from, today’s remarkable chatbots emerged.
</p><h2>How do large language models work?</h2><p>
	A transformer-based LLM is trained by giving it a vast dataset of text to learn from. The attention mechanism comes into play as it processes sentences and looks for patterns. By looking at all the words in a sentence at once, it gradually begins to understand which words are most commonly found together and which words are most important to the meaning of the sentence. It learns these things by trying to predict the next word in a sentence and comparing its guess to the ground truth. Its errors act as feedback signals that cause the model to adjust the weights it assigns to various words before it tries again.
</p><p class="shortcode-media shortcode-media-rebelmouse-image rm-resized-container rm-resized-container-25 rm-float-left" style="float: left;">
<img alt="A chart shows the size of five LLMs in parameters and their performance on a benchmark." class="rm-shortcode rm-resized-image" id="91ba0" src="https://spectrum.ieee.org/media-library/a-chart-shows-the-size-of-five-llms-in-parameters-and-their-performance-on-a-benchmark.png?id=51455578&amp;width=980" />
<small class="image-media media-caption">These five LLMs vary greatly in size (given in parameters), and the larger models have better performance on a standard LLM benchmark test. </small><small class="image-media media-photo-credit">IEEE Spectrum</small>
</p><p>
	To explain the training process in slightly more technical terms, the text in the training data is broken down into elements called 
	<a href="https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/" rel="noopener noreferrer" target="_blank">tokens</a>, which are words or pieces of words—but for simplicity’s sake, let’s say all tokens are words. As the model goes through the sentences in its training data and learns the relationships between tokens, it creates a list of numbers, called a vector, for each one. All the numbers in the vector represent various aspects of the word: its semantic meanings, its relationship to other words, its frequency of use, and so on. Similar words, like <em>elegant </em>and <em>fancy</em>, will have similar vectors and will also be near each other in the vector space. These vectors are called word embeddings. The parameters of an LLM include the weights associated with all the word embeddings and the attention mechanism. <a href="https://spectrum.ieee.org/tag/gpt-4" target="_blank">GPT-4</a>, the OpenAI model that’s considered the current champion, is rumored to have more than 1 trillion parameters. </p><p>
	Given enough data and training time, the LLM begins to understand the subtleties of language. While much of the training involves looking at text sentence by sentence, the attention mechanism also captures relationships between words throughout a longer text sequence of many paragraphs. Once an LLM is trained and is ready for use, the attention mechanism is still in play. When the model is generating text in response to a prompt, it’s using its predictive powers to decide what the next word should be. When generating longer pieces of text, it predicts the next word in the context of all the words it has written so far; this function increases the coherence and continuity of its writing.
</p><h2>Why do large language models hallucinate?</h2><p>
	You may have heard that LLMs sometimes “<a href="https://spectrum.ieee.org/ai-hallucination" target="_self">hallucinate</a>.” That’s a polite way to say they make stuff up very convincingly. A model sometimes generates text that fits the context and is grammatically correct, yet the material is erroneous or nonsensical. This bad habit stems from LLMs training on vast troves of data drawn from the Internet, plenty of which is not factually accurate. Since the model is simply trying to predict the next word in a sequence based on what it has seen, it may generate plausible-sounding text that has no grounding in reality.
</p><h2>Why is generative AI controversial?</h2><p>
	One source of controversy for generative AI is the provenance of its training data. Most AI companies that train large models to generate text, images, video, and audio have 
	<a href="https://spectrum.ieee.org/ai-ethics" target="_self">not been transparent</a> about the content of their training datasets. Various leaks and experiments have revealed that those datasets <a href="https://spectrum.ieee.org/midjourney-copyright" target="_self">include copyrighted material</a> such as books, newspaper articles, and movies. A <a href="https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30" rel="noopener noreferrer" target="_blank">number</a> of <a href="https://www.reuters.com/legal/john-grisham-other-top-us-authors-sue-openai-over-copyrights-2023-09-20/" rel="noopener noreferrer" target="_blank">lawsuits</a> are underway to determine whether <a href="https://spectrum.ieee.org/generative-ai-ip-problem" target="_self">use of copyrighted material</a> for training AI systems constitutes fair use, or whether the AI companies need to pay the copyright holders for use of their material.
</p><p>
	On a related note, many people are concerned that the widespread use of generative AI <a href="https://spectrum.ieee.org/ai-taking-over-jobs" target="_blank">will take jobs away</a> from creative humans who make art, music, written works, and so forth. People are also concerned that it could take jobs from humans who do a wide range of white-collar jobs, including translators, paralegals, customer-service representatives, and journalists. There have already been a few 
	<a href="https://www.washingtonpost.com/technology/2024/01/10/duolingo-ai-layoffs/" rel="noopener noreferrer" target="_blank">troubling layoffs</a>, but it’s hard to say yet whether generative AI will be reliable enough for large-scale enterprise applications. (See above about hallucinations.)</p><p>
	Finally, there’s the danger that generative AI will be used to make bad stuff. And there are of course many categories of bad stuff it could theoretically be used for. Generative AI can be used for personalized scams and phishing attacks: For example, using “voice cloning,” scammers can 
	<a href="https://www.cbsnews.com/news/scammers-ai-mimic-voices-loved-ones-in-distress/" rel="noopener noreferrer" target="_blank">copy the voice of a specific person</a> and call the person’s family with a plea for help (and money). All formats of generative AI—text, audio, image, and video—can be used to generate misinformation by creating plausible-seeming representations of things that never happened, which is a particularly worrying possibility when it comes to <a href="https://spectrum.ieee.org/deepfakes-election" target="_self">elections</a>. (Meanwhile, as <em>IEEE </em><em>Spectrum</em> reported this week, the U.S. Federal Communications Commission has responded by <a href="https://spectrum.ieee.org/ai-robocalls-2667266649" target="_blank">outlawing AI-generated robocalls</a>.) Image- and video-generating tools can be used to produce nonconsensual pornography, although the tools made by mainstream companies disallow such use. And chatbots can theoretically walk a would-be terrorist through the steps of making a bomb, nerve gas, and a host of other horrors. Although the big LLMs have safeguards to prevent such misuse, some hackers delight in circumventing those safeguards. What’s more, “uncensored” versions of <a href="https://spectrum.ieee.org/open-source-ai-2666932122" target="_self">open-source LLMs</a> are out there.
</p><p>
	Despite such potential problems, many people think that generative AI can also make people more productive and could be used as a tool to enable entirely new forms of creativity. We’ll likely see both disasters and creative flowerings and plenty else that we don’t expect. But knowing the basics of how these models work is increasingly crucial for tech-savvy people today. Because no matter how sophisticated these systems grow, it’s the humans’ job to keep them running, make the next ones better, and with any luck, help people out too.
</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Everything You Wanted to Know About 1X’s Latest Video</h3><p><a href="https://spectrum.ieee.org/1x-robotics-video">https://spectrum.ieee.org/1x-robotics-video</a></p><p><img src="https://spectrum.ieee.org/media-library/two-humanoid-robots-stand-at-a-workbenches-with-two-humanoid-robots-in-the-background.png?id=51444298&amp;width=1245&amp;height=700&amp;coordinates=0%2C0%2C0%2C0" /><br /><br /><p>Just last month, Oslo-based 1X (formerly Halodi Robotics) announced a massive US $100 million Series B, and clearly it has been putting the work in. <a href="https://www.youtube.com/watch?v=iHXuU3nTXfQ" target="_blank">A new video posted last week</a> shows a [insert <a href="https://en.wikipedia.org/wiki/Collective_noun" target="_blank">collective noun</a> for humanoid robots here] of EVE android-ish mobile manipulators doing <a href="https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed" target="_blank">a wide variety of tasks leveraging end-to-end neural networks</a> (pixels to actions). And best of all, the video seems to be more or less an honest one: a single take, at (appropriately) 1X speed, and full autonomy. But we still had questions! And 1X has answers.</p><hr /><p class="shortcode-media shortcode-media-youtube">
<span class="rm-shortcode" style="display: block; padding-top: 56.25%;"></span>
</p><p>If, like me, you had some very important questions after watching this video, including whether that plant is actually dead and the fate of the weighted companion cube, you’ll want to read this Q&amp;A with <a href="https://twitter.com/ericjang11" target="_blank">Eric Jang</a>, vice president of artificial intelligence at 1X.</p><p><strong>How many takes did it take to get this take?</strong></p><p><strong>Eric Jang: </strong>About 10 takes that lasted more than a minute; this was our first time doing a video like this, so it was more about learning how to coordinate the film crew and set up the shoot to look impressive.</p><p><strong>Did you train your robots specifically on floppy things and transparent things?</strong></p><p><strong>Jang: </strong>Nope! We train our neural network to pick up all kinds of objects—both rigid and deformable and transparent things. Because we train manipulation end-to-end from pixels, picking up deformables and transparent objects is much easier than a classical grasping pipeline, where you have to figure out the exact geometry of what you are trying to grasp.</p><p><strong>What keeps your robots from doing these tasks faster?</strong></p><p><strong>Jang: </strong>Our robots learn from demonstrations, so they go at exactly the same speed the human teleoperators demonstrate the task at. If we gathered demonstrations where we move faster, so would the robots.</p><p><strong>How many <a href="https://theportalwiki.com/wiki/Weighted_Companion_Cube" rel="noopener noreferrer" target="_blank">weighted companion cubes</a> were harmed in the making of this video?</strong></p><p><strong>Jang: </strong>At 1X, weighted companion cubes do not have rights.</p><p><strong>That’s a very cool method for charging, but it seems a lot more complicated than some kind of drive-on interface directly with the base. Why use manipulation instead?</strong></p><p><strong>Jang: </strong>You’re right that this isn’t the simplest way to charge the robot, but if we are going to succeed at our mission to build generally capable and reliable robots that can manipulate all kinds of objects, our neural nets have to be able to do this task at the very least. Plus, it reduces costs quite a bit and simplifies the system!</p><p><strong>What animal is that blue plush supposed to be?</strong></p><p><strong>Jang: </strong>It’s an obese shark, I think.</p><p><strong>How many different robots are in this video?</strong></p><p><strong>Jang: </strong>17? And more that are stationary.</p><p><strong>How do you tell the robots apart?</strong></p><p><strong>Jang: </strong>They have little numbers printed on the base.</p><p><strong>Is that plant dead?</strong></p><p><strong>Jang: </strong>Yes, we put it there because no CGI/3D-rendered video would ever go through the trouble of adding a dead plant.</p><p><strong>What sort of existential crisis is the robot at the window having?</strong></p><p> <strong>Jang: </strong>It was <a href="https://twitter.com/ericjang11/status/1756138725363662948" target="_blank">supposed to be</a> opening and closing the window repeatedly (good for testing statistical significance).</p><p><strong>If one of the robots was actually a human in a helmet and a suit holding grippers and standing on a mobile base, would I be able to tell?</strong></p><p><strong>Jang: </strong>I was super flattered by this comment on the Youtube video:</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" id="0d42f" src="https://spectrum.ieee.org/media-library/image.png?id=51444288&amp;width=980" />
</p><p>But if you look at the area where the upper arm tapers at the shoulder, it’s too thin for a human to fit inside while still having such broad shoulders:</p><p class="shortcode-media shortcode-media-rebelmouse-image">
<img alt="" class="rm-shortcode" id="35356" src="https://spectrum.ieee.org/media-library/image.png?id=51444290&amp;width=980" />
</p><p><strong>Why are your robots so happy all the time? Are you planning to do more complex HRI (human-robot interaction) stuff with their faces?</strong></p><p><strong>Jang: </strong>Yes, more complex HRI stuff is in the pipeline! </p><p><strong>Are your robots able to autonomously collaborate with each other?</strong></p><p><strong>Jang: </strong>Stay tuned! </p><p><strong>Is the <a href="https://mathworld.wolfram.com/SkewPolyomino.html" target="_blank">skew tetromino</a><a href="https://en.wikipedia.org/wiki/Tetromino" target="_blank"></a> the most difficult <a href="https://en.wikipedia.org/wiki/Tetromino" target="_blank">tetromino</a> for robotic manipulation?</strong></p><p><strong>Jang: </strong>Good catch! Yes, the green one is the worst of them all because there are many valid ways to pinch it with the gripper and lift it up. In robotic learning, if there are multiple ways to pick something up, it can actually confuse the machine learning model. Kind of like asking a car to turn left and right at the same time to avoid a tree. </p><p><strong>Everyone else’s robots are making coffee. Can your robots make coffee?</strong></p><p><strong>Jang: </strong><a href="https://x.com/ericjang11/status/1755666730326823168?s=20" target="_blank">Yep!</a> We were planning to throw in some coffee making on this video as an Easter egg, but the coffee machine broke right before the film shoot and it turns out it’s impossible to get a Keurig K-Slim in Norway via next-day shipping.</p><div class="horizontal-rule"></div><p>1X is currently hiring both AI researchers (specialties include imitation learning, reinforcement learning, and large-scale training) and android operators (!) which actually sounds like a super fun and interesting job. <a href="https://www.1x.tech/discover/all-neural-networks-all-autonomous-all-1x-speed" target="_blank">More here</a>.</p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Andrew Ng: Unbiggen AI</h3><p><a href="https://spectrum.ieee.org/andrew-ng-data-centric-ai">https://spectrum.ieee.org/andrew-ng-data-centric-ai</a></p><p><img src="https://spectrum.ieee.org/media-library/andrew-ng-listens-during-the-power-of-data-sooner-than-you-think-global-technology-conference-in-brooklyn-new-york-on-wednes.jpg?id=29206806&amp;width=1245&amp;height=700&amp;coordinates=0%2C0%2C0%2C474" /><br /><br /><p><strong><a href="https://en.wikipedia.org/wiki/Andrew_Ng" rel="noopener noreferrer" target="_blank">Andrew Ng</a> has serious street cred</strong> in artificial intelligence. He pioneered the use of graphics processing units (GPUs) to train deep learning models in the late 2000s with his students at <a href="https://stanfordmlgroup.github.io/" rel="noopener noreferrer" target="_blank">Stanford University</a>, cofounded <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> in 2011, and then served for three years as chief scientist for <a href="https://ir.baidu.com/" rel="noopener noreferrer" target="_blank">Baidu</a>, where he helped build the Chinese tech giant’s AI group. So when he says he has identified the next big shift in artificial intelligence, people listen. And that’s what he told <em>IEEE Spectrum</em> in an exclusive Q&amp;A.</p><hr /><p>
	Ng’s current efforts are focused on his company 
	<a href="https://landing.ai/about/" rel="noopener noreferrer" target="_blank">Landing AI</a>, which built a platform called LandingLens to help manufacturers improve visual inspection with computer vision. <a name="top"></a>He has also become something of an evangelist for what he calls the <a href="https://www.youtube.com/watch?v=06-AZXmwHjo" target="_blank">data-centric AI movement</a>, which he says can yield “small data” solutions to big issues in AI, including model efficiency, accuracy, and bias.
</p><p>
	Andrew Ng on...
</p><ul>
<li><a href="#big">What’s next for really big models</a></li>
<li><a href="#career">The career advice he didn’t listen to</a></li>
<li><a href="#defining">Defining the data-centric AI movement</a></li>
<li><a href="#synthetic">Synthetic data</a></li>
<li><a href="#work">Why Landing AI asks its customers to do the work</a></li>
</ul><p>
<a name="big"></a><strong>The great advances in deep learning over the past decade or so have been powered by ever-bigger models crunching ever-bigger amounts of data. Some people argue that that’s an <a href="https://spectrum.ieee.org/deep-learning-computational-cost" target="_self">unsustainable trajectory</a>. Do you agree that it can’t go on that way?</strong>
</p><p>
<strong>Andrew Ng: </strong>This is a big question. We’ve seen foundation models in NLP [natural language processing]. I’m excited about NLP models getting even bigger, and also about the potential of building foundation models in computer vision. I think there’s lots of signal to still be exploited in video: We have not been able to build foundation models yet for video because of compute bandwidth and the cost of processing video, as opposed to tokenized text. So I think that this engine of scaling up deep learning algorithms, which has been running for something like 15 years now, still has steam in it. Having said that, it only applies to certain problems, and there’s a set of other problems that need small data solutions.
</p><p>
<strong>When you say you want a foundation model for computer vision, what do you mean by that?</strong>
</p><p>
<strong>Ng:</strong> This is a term coined by <a href="https://cs.stanford.edu/~pliang/" rel="noopener noreferrer" target="_blank">Percy Liang</a> and <a href="https://crfm.stanford.edu/" rel="noopener noreferrer" target="_blank">some of my friends at Stanford</a> to refer to very large models, trained on very large data sets, that can be tuned for specific applications. For example, <a href="https://spectrum.ieee.org/open-ais-powerful-text-generating-tool-is-ready-for-business" target="_self">GPT-3</a> is an example of a foundation model [for NLP]. Foundation models offer a lot of promise as a new paradigm in developing machine learning applications, but also challenges in terms of making sure that they’re reasonably fair and free from bias, especially if many of us will be building on top of them.
</p><p>
<strong>What needs to happen for someone to build a foundation model for video?</strong>
</p><p>
<strong>Ng:</strong> I think there is a scalability problem. The compute power needed to process the large volume of images for video is significant, and I think that’s why foundation models have arisen first in NLP. Many researchers are working on this, and I think we’re seeing early signs of such models being developed in computer vision. But I’m confident that if a semiconductor maker gave us 10 times more processor power, we could easily find 10 times more video to build such models for vision.
</p><p>
	Having said that, a lot of what’s happened over the past decade is that deep learning has happened in consumer-facing companies that have large user bases, sometimes billions of users, and therefore very large data sets. While that paradigm of machine learning has driven a lot of economic value in consumer software, I find that that recipe of scale doesn’t work for other industries.
</p><p>
<a href="#top">Back to top</a><a name="career"></a>
</p><p>
<strong>It’s funny to hear you say that, because your early work was at a consumer-facing company with millions of users.</strong>
</p><p>
<strong>Ng: </strong>Over a decade ago, when I proposed starting the <a href="https://research.google/teams/brain/" rel="noopener noreferrer" target="_blank">Google Brain</a> project to use Google’s compute infrastructure to build very large neural networks, it was a controversial step. One very senior person pulled me aside and warned me that starting Google Brain would be bad for my career. I think he felt that the action couldn’t just be in scaling up, and that I should instead focus on architecture innovation.
</p><p class="pull-quote">
	“In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.”<br />
	—Andrew Ng, CEO & Founder, Landing AI
</p><p>
	I remember when my students and I published the first 
	<a href="https://nips.cc/" rel="noopener noreferrer" target="_blank">NeurIPS</a> workshop paper advocating using <a href="https://developer.nvidia.com/cuda-zone" rel="noopener noreferrer" target="_blank">CUDA</a>, a platform for processing on GPUs, for deep learning—a different senior person in AI sat me down and said, “CUDA is really complicated to program. As a programming paradigm, this seems like too much work.” I did manage to convince him; the other person I did not convince.
</p><p>
<strong>I expect they’re both convinced now.</strong>
</p><p>
<strong>Ng:</strong> I think so, yes.
</p><p>
	Over the past year as I’ve been speaking to people about the data-centric AI movement, I’ve been getting flashbacks to when I was speaking to people about deep learning and scalability 10 or 15 years ago. In the past year, I’ve been getting the same mix of “there’s nothing new here” and “this seems like the wrong direction.”
</p><p>
<a href="#top">Back to top</a><a name="defining"></a>
</p><p>
<strong>How do you define data-centric AI, and why do you consider it a movement?</strong>
</p><p>
<strong>Ng:</strong> Data-centric AI is the discipline of systematically engineering the data needed to successfully build an AI system. For an AI system, you have to implement some algorithm, say a neural network, in code and then train it on your data set. The dominant paradigm over the last decade was to download the data set while you focus on improving the code. Thanks to that paradigm, over the last decade deep learning networks have improved significantly, to the point where for a lot of applications the code—the neural network architecture—is basically a solved problem. So for many practical applications, it’s now more productive to hold the neural network architecture fixed, and instead find ways to improve the data.
</p><p>
	When I started speaking about this, there were many practitioners who, completely appropriately, raised their hands and said, “Yes, we’ve been doing this for 20 years.” This is the time to take the things that some individuals have been doing intuitively and make it a systematic engineering discipline.
</p><p>
	The data-centric AI movement is much bigger than one company or group of researchers. My collaborators and I organized a 
	<a href="https://neurips.cc/virtual/2021/workshop/21860" rel="noopener noreferrer" target="_blank">data-centric AI workshop at NeurIPS</a>, and I was really delighted at the number of authors and presenters that showed up.
</p><p>
<strong>You often talk about companies or institutions that have only a small amount of data to work with. How can data-centric AI help them?</strong>
</p><p>
<strong>Ng: </strong>You hear a lot about vision systems built with millions of images—I once built a face recognition system using 350 million images. Architectures built for hundreds of millions of images don’t work with only 50 images. But it turns out, if you have 50 really good examples, you can build something valuable, like a defect-inspection system. In many industries where giant data sets simply don’t exist, I think the focus has to shift from big data to good data. Having 50 thoughtfully engineered examples can be sufficient to explain to the neural network what you want it to learn.
</p><p>
<strong>When you talk about training a model with just 50 images, does that really mean you’re taking an existing model that was trained on a very large data set and fine-tuning it? Or do you mean a brand new model that’s designed to learn only from that small data set?</strong>
</p><p>
<strong>Ng: </strong>Let me describe what Landing AI does. When doing visual inspection for manufacturers, we often use our own flavor of <a href="https://developers.arcgis.com/python/guide/how-retinanet-works/" rel="noopener noreferrer" target="_blank">RetinaNet</a>. It is a pretrained model. Having said that, the pretraining is a small piece of the puzzle. What’s a bigger piece of the puzzle is providing tools that enable the manufacturer to pick the right set of images [to use for fine-tuning] and label them in a consistent way. There’s a very practical problem we’ve seen spanning vision, NLP, and speech, where even human annotators don’t agree on the appropriate label. For big data applications, the common response has been: If the data is noisy, let’s just get a lot of data and the algorithm will average over it. But if you can develop tools that flag where the data’s inconsistent and give you a very targeted way to improve the consistency of the data, that turns out to be a more efficient way to get a high-performing system.
</p><p class="pull-quote">
	“Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.”<br />
	—Andrew Ng
</p><p>
	For example, if you have 10,000 images where 30 images are of one class, and those 30 images are labeled inconsistently, one of the things we do is build tools to draw your attention to the subset of data that’s inconsistent. So you can very quickly relabel those images to be more consistent, and this leads to improvement in performance.
</p><p>
<strong>Could this focus on high-quality data help with bias in data sets? If you’re able to curate the data more before training?</strong>
</p><p>
<strong>Ng:</strong> Very much so. Many researchers have pointed out that biased data is one factor among many leading to biased systems. There have been many thoughtful efforts to engineer the data. At the NeurIPS workshop, <a href="https://www.cs.princeton.edu/~olgarus/" rel="noopener noreferrer" target="_blank">Olga Russakovsky</a> gave a really nice talk on this. At the main NeurIPS conference, I also really enjoyed <a href="https://neurips.cc/virtual/2021/invited-talk/22281" rel="noopener noreferrer" target="_blank">Mary Gray’s presentation,</a> which touched on how data-centric AI is one piece of the solution, but not the entire solution. New tools like <a href="https://www.microsoft.com/en-us/research/project/datasheets-for-datasets/" rel="noopener noreferrer" target="_blank">Datasheets for Datasets</a> also seem like an important piece of the puzzle.
</p><p>
	One of the powerful tools that data-centric AI gives us is the ability to engineer a subset of the data. Imagine training a machine-learning system and finding that its performance is okay for most of the data set, but its performance is biased for just a subset of the data. If you try to change the whole neural network architecture to improve the performance on just that subset, it’s quite difficult. But if you can engineer a subset of the data you can address the problem in a much more targeted way.
</p><p>
<strong>When you talk about engineering the data, what do you mean exactly?</strong>
</p><p>
<strong>Ng: </strong>In AI, data cleaning is important, but the way the data has been cleaned has often been in very manual ways. In computer vision, someone may visualize images through a <a href="https://jupyter.org/" rel="noopener noreferrer" target="_blank">Jupyter notebook</a> and maybe spot the problem, and maybe fix it. But I’m excited about tools that allow you to have a very large data set, tools that draw your attention quickly and efficiently to the subset of data where, say, the labels are noisy. Or to quickly bring your attention to the one class among 100 classes where it would benefit you to collect more data. Collecting more data often helps, but if you try to collect more data for everything, that can be a very expensive activity.
</p><p>
	For example, I once figured out that a speech-recognition system was performing poorly when there was car noise in the background. Knowing that allowed me to collect more data with car noise in the background, rather than trying to collect more data for everything, which would have been expensive and slow.
</p><p>
<a href="#top">Back to top</a><a name="synthetic"></a>
</p><p>
<strong>What about using synthetic data, is that often a good solution?</strong>
</p><p>
<strong>Ng: </strong>I think synthetic data is an important tool in the tool chest of data-centric AI. At the NeurIPS workshop, <a href="https://tensorlab.cms.caltech.edu/users/anima/" rel="noopener noreferrer" target="_blank">Anima Anandkumar</a> gave a great talk that touched on synthetic data. I think there are important uses of synthetic data that go beyond just being a preprocessing step for increasing the data set for a learning algorithm. I’d love to see more tools to let developers use synthetic data generation as part of the closed loop of iterative machine learning development.
</p><p>
<strong>Do you mean that synthetic data would allow you to try the model on more data sets?</strong>
</p><p>
<strong>Ng: </strong>Not really. Here’s an example. Let’s say you’re trying to detect defects in a smartphone casing. There are many different types of defects on smartphones. It could be a scratch, a dent, pit marks, discoloration of the material, other types of blemishes. If you train the model and then find through error analysis that it’s doing well overall but it’s performing poorly on pit marks, then synthetic data generation allows you to address the problem in a more targeted way. You could generate more data just for the pit-mark category.
</p><p class="pull-quote">
	“In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models.”<br />
	—Andrew Ng
</p><p>
	Synthetic data generation is a very powerful tool, but there are many simpler tools that I will often try first. Such as data augmentation, improving labeling consistency, or just asking a factory to collect more data.
</p><p>
<a href="#top">Back to top</a><a name="work"></a>
</p><p>
<strong>To make these issues more concrete, can you walk me through an example? When a company approaches <a href="https://landing.ai/" rel="noopener noreferrer" target="_blank">Landing AI</a> and says it has a problem with visual inspection, how do you onboard them and work toward deployment?</strong>
</p><p>
<strong>Ng: </strong>When a customer approaches us we usually have a conversation about their inspection problem and look at a few images to verify that the problem is feasible with computer vision. Assuming it is, we ask them to upload the data to the <a href="https://landing.ai/platform/" rel="noopener noreferrer" target="_blank">LandingLens</a> platform. We often advise them on the methodology of data-centric AI and help them label the data.
</p><p>
	One of the foci of Landing AI is to empower manufacturing companies to do the machine learning work themselves. A lot of our work is making sure the software is fast and easy to use. Through the iterative process of machine learning development, we advise customers on things like how to train models on the platform, when and how to improve the labeling of data so the performance of the model improves. Our training and software supports them all the way through deploying the trained model to an edge device in the factory.
</p><p>
<strong>How do you deal with changing needs? If products change or lighting conditions change in the factory, can the model keep up?</strong>
</p><p>
<strong>Ng:</strong> It varies by manufacturer. There is data drift in many contexts. But there are some manufacturers that have been running the same manufacturing line for 20 years now with few changes, so they don’t expect changes in the next five years. Those stable environments make things easier. For other manufacturers, we provide tools to flag when there’s a significant data-drift issue. I find it really important to empower manufacturing customers to correct data, retrain, and update the model. Because if something changes and it’s 3 a.m. in the United States, I want them to be able to adapt their learning algorithm right away to maintain operations.
</p><p>
	In the consumer software Internet, we could train a handful of machine-learning models to serve a billion users. In manufacturing, you might have 10,000 manufacturers building 10,000 custom AI models. The challenge is, how do you do that without Landing AI having to hire 10,000 machine learning specialists?
</p><p>
<strong>So you’re saying that to make it scale, you have to empower customers to do a lot of the training and other work.</strong>
</p><p>
<strong>Ng: </strong>Yes, exactly! This is an industry-wide problem in AI, not just in manufacturing. Look at health care. Every hospital has its own slightly different format for electronic health records. How can every hospital train its own custom AI model? Expecting every hospital’s IT personnel to invent new neural-network architectures is unrealistic. The only way out of this dilemma is to build tools that empower the customers to build their own models by giving them tools to engineer the data and express their domain knowledge. That’s what Landing AI is executing in computer vision, and the field of AI needs other teams to execute this in other domains.
</p><p>
<strong>Is there anything else you think it’s important for people to understand about the work you’re doing or the data-centric AI movement?</strong>
</p><p>
<strong>Ng: </strong>In the last decade, the biggest shift in AI was a shift to deep learning. I think it’s quite possible that in this decade the biggest shift will be to data-centric AI. With the maturity of today’s neural network architectures, I think for a lot of the practical applications the bottleneck will be whether we can efficiently get the data we need to develop systems that work well. The data-centric AI movement has tremendous energy and momentum across the whole community. I hope more researchers and developers will jump in and work on it.
</p><p>
<a href="#top">Back to top</a>
</p><p><em>This article appears in the April 2022 print issue as “Andrew Ng, AI Minimalist</em><em>.”</em></p></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Major Shareholders Planning to Force Apple to Reveal Use of AI</h3><p><a href="https://www.macrumors.com/2024/02/27/shareholders-to-force-apple-to-reveal-ai-use/">https://www.macrumors.com/2024/02/27/shareholders-to-force-apple-to-reveal-ai-use/</a></p><p>Some of Apple's biggest investors are set to pressure the company tomorrow to reveal its use of artificial intelligence tools (via the <a href="https://www.ft.com/content/387c446c-55e0-4f54-a4b4-80bf9d5bc2f3"><em>Financial Times</em></a>).
<br />

<br />
<img alt="" class="alignnone size-full wp-image-840591" height="630" src="https://images.macrumors.com/article-new/2022/03/hey-siri-banner-apple.jpg" width="1200" />
<br />
Apple's annual shareholder meeting takes place tomorrow, allowing those with a major stake in the company to put forward proposals. One resolution proposed by the American Federation of Labor and Congress of Industrial Organizations (AFL-CIO) asks Apple to disclose its use of AI and any ethical guidelines that the company has adopted regarding the technology.
<br />

<br />
The resolution is set to be supported by Norges Bank Investment Management and Legal & General, Apple's eighth and 10th-largest shareholders. Norges Bank, which operates the world's biggest sovereign wealth fund, wrote in its voting disclosures that Apple's board should account for "social consequences of its operations and products." Likewise, Legal & General said that Apple "discloses very little about its approach to managing AI-related risks."
<br />

<br />
Legal & General met with Apple to discuss AI, but it declined to increase transparency around its development and use of the technology. "Apple should be transparent in their uses of AI and their risk management processes," the company said.
<br />

<br />
The major investor advisory firm Institutional Shareholder Services is encouraging Apple investors to support the AI resolution, arguing that Apple's guidelines "do not specifically identify the potential risks resulting from the use of AI" and, as a result, "there are concerns regarding shareholders' ability to properly evaluate the risks associated with the use of AI."
<br />

<br />
Meanwhile, Apple is urging investors to reject the resolution, claiming that "the scope of the requested report is overly broad and could encompass disclosure of strategic plans and initiatives harmful to our competitive position." Shareholder petitions in the United States are usually non-binding, but those that review support from more than 30% of investors usually put sufficient pressure on the company to act. Apple is widely expected to announce a range of new AI features for its devices at WWDC later this year.<div class="linkback">Tags: <a href="https://www.macrumors.com/guide/aapl/">AAPL</a>, <a href="https://www.macrumors.com/guide/artificial-intelligence/">Artificial Intelligence</a></div><br />This article, &quot;<a href="https://www.macrumors.com/2024/02/27/shareholders-to-force-apple-to-reveal-ai-use/">Major Shareholders Planning to Force Apple to Reveal Use of AI</a>&quot; first appeared on <a href="https://www.macrumors.com">MacRumors.com</a><br /><br /><a href="https://forums.macrumors.com/threads/major-shareholders-planning-to-force-apple-to-reveal-use-of-ai.2420535/">Discuss this article</a> in our forums<br /><br /></p><p>---------------------------------</p></body>
                                </html>
                                <h3>Tyler Perry puts $800 million studio expansion on hold because of OpenAI’s Sora</h3><p><a href="https://arstechnica.com/?p=2005529">https://arstechnica.com/?p=2005529</a></p><p>Perry: Mind-blowing AI video-generation tools "will touch every corner of our industry."</p><p>---------------------------------</p></body>
                                </html>
                                <h3>OpenAI collapses media reality with Sora, a photorealistic AI video generator</h3><p><a href="https://arstechnica.com/?p=2003861">https://arstechnica.com/?p=2003861</a></p><p>Hello, cultural singularity—soon, every video you see online could be completely fake.</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Corporate Responsibility in the Age of AI</h3><p><a href="https://www.oreilly.com/radar/corporate-responsibility-in-the-age-of-ai/">https://www.oreilly.com/radar/corporate-responsibility-in-the-age-of-ai/</a></p><p>Since its release in November 2022, almost everyone involved with technology has experimented with ChatGPT: students, faculty, and professionals in almost every discipline. Almost every company has undertaken AI projects, including companies that, at least on the face of it, have “no AI” policies. Last August, OpenAI stated that 80% of Fortune 500 companies have [&#8230;]</p><p>---------------------------------</p></body>
                                </html>
                                <h3>The OpenAI Endgame</h3><p><a href="https://www.oreilly.com/radar/the-openai-endgame/">https://www.oreilly.com/radar/the-openai-endgame/</a></p><p>Since the New York Times sued OpenAI for infringing its copyrights by using Times content for training, everyone involved with AI has been wondering about the consequences. How will this lawsuit play out? And, more importantly, how will the outcome affect the way we train and use large language models? There are two components to [&#8230;]</p><p>---------------------------------</p></body>
                                </html>
                                <h3>Strawberry Fields Forever</h3><p><a href="https://www.oreilly.com/radar/strawberry-fields-forever/">https://www.oreilly.com/radar/strawberry-fields-forever/</a></p><p>Tim O’Reilly forwarded an excellent article about the OpenAI soap opera to me: Matt Levine’s “Money Stuff: Who Controls Open AI.” I’ll skip most of it, but something caught my eye. Toward the end, Levine writes about Elon Musk’s version of Nick Bostrom’s AI that decides to turn the world to paperclips: [Elon] Musk gave [&#8230;]</p><p>---------------------------------</p></body>
                                </html>
                                <h3>AI comes for jobs at studio of American filmmaker Tyler Perry</h3><p><a href="https://go.theregister.com/feed/www.theregister.com/2024/02/23/ai_jobs_tyler_perry/">https://go.theregister.com/feed/www.theregister.com/2024/02/23/ai_jobs_tyler_perry/</a></p><p><h4>$800M expansion canceled after a mere glimpse at the power of Sora</h4> <p>If you ask American film mogul Tyler Perry, AI isn't coming for jobs – it's already taken them. Case in point, Perry's Atlanta film studio, where the movie maker just scrapped an expansion in the works for four years after getting a glimpse of OpenAI's Sora. …</p></p><p>---------------------------------</p></body>
                                </html>
                                